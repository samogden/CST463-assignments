{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cm-Ca2tINtDv"
   },
   "source": [
    "# Logistic regression via gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "There are 6 problems (look for Problem 1, Problem 2, etc.). For each, replace the comment YOUR CODE HERE with your code. Do not modify the code in other ways. \n",
    "\n",
    "You will probably want to write test code while you're getting your code working. Mark your test code clearly and then remove it before submitting. \n",
    "\n",
    "Before submitting your code, restart the kernel and then run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1599418996517,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "yZabxQdKDlmv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random seed is set so results are repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTbqZrhKKByW"
   },
   "source": [
    "## Gradients\n",
    "\n",
    "The code used here to compute the gradient of a function is different from the code used in the linear regression assignment.  This code is more efficient.  Please read the code carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partials_at_x(f, n, x):\n",
    "    h = 0.000001\n",
    "    y = f(x)\n",
    "    partial_derivs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x[i] += h\n",
    "        yh = f(x)\n",
    "        x[i] -= h\n",
    "        partial_derivs[i] = yh - y\n",
    "    partial_derivs = partial_derivs/h\n",
    "    return partial_derivs\n",
    "    \n",
    "def gradient(f, n):\n",
    "    return lambda x: partials_at_x(f, n, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1599418996521,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "8W5rWPnrKgsv",
    "outputId": "82a46fc8-a326-42cd-92b8-4dd4fd1c795a"
   },
   "outputs": [],
   "source": [
    "# test the gradient() function\n",
    "\n",
    "def f(x):\n",
    "    return 2*x[0]**2 + x[0]*x[1]\n",
    "\n",
    "# print the gradient of f\n",
    "f_grad = gradient(f, 2)\n",
    "xs = [ np.array(x) for x in [[1.0, 2.0], [0.0, 0.0], [2.0, -1.0]] ]\n",
    "for x in xs:\n",
    "    print('x: {}, f_grad(x): {}'.format(x, f_grad(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xv6-zzJ-_FSV"
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "In gradient descent we want to find the value of x that minimizes (or maximizes) a function f.  We do this by starting with some x, computing the value of the gradient of f at x, and then using that value to make an adjustment to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1204,
     "status": "ok",
     "timestamp": 1599418996915,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "kLFYL_3U_JY9"
   },
   "outputs": [],
   "source": [
    "def grad_descent(f, n, alpha=0.1, n_iterations=1000, threshold=0.001):\n",
    "  \"\"\" Return the value x for which function f(x) is minimum, and also\n",
    "  the values of x along the path of gradient descent.  \n",
    "  The algorithm terminates when iteration limit is reach or when the\n",
    "  L1 norm of the change to x is below the threshold.  \"\"\"\n",
    "  \n",
    "  f_grad = gradient(f, n)\n",
    "\n",
    "  x = np.random.rand(n)           # note random starting point\n",
    "  xs = [x.copy()]\n",
    "  for i in range(n_iterations):\n",
    "    delta = alpha * f_grad(x)\n",
    "    x -= delta\n",
    "    xs.append(x.copy())           # the copy is needed, else all values in xs will be the same\n",
    "    if np.sum(np.absolute(delta)) < threshold:     # L1 norm\n",
    "      return x, xs\n",
    "\n",
    "  print('warning: reached iteration limit')\n",
    "  return x, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1599418996916,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "w9DKGU8YALJe",
    "outputId": "6298f0d0-122f-4e40-bf0e-96d6d2acc84d"
   },
   "outputs": [],
   "source": [
    "# test the grad_descent() function\n",
    "\n",
    "# test on some 1D functions\n",
    "f1 = lambda x: (x[0] - 1)**2\n",
    "f2 = lambda x: 0.2*(x[0]**2) + np.sin(x[0])\n",
    "for f in [f1, f2]:\n",
    "  x, xs = grad_descent(f, 1)\n",
    "  print('x = {}, f(x) = {:0.3f}'.format(x, f(x)))\n",
    "    \n",
    "# test on 2D function\n",
    "f = lambda x: (x[0]-1)**2 + (x[1]+1)**2 + 2.0\n",
    "x, xs = grad_descent(f, 2)\n",
    "print('x = {}, f(x) = {:0.3f}'.format(x, f(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the value of the loss function as gradient descent proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1599418996916,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "qgELkT8-IhUH"
   },
   "outputs": [],
   "source": [
    "def plot_descent(f, xs):\n",
    "  \"\"\" plot value of function f using history xs of gradient descent \"\"\"\n",
    "  plt.plot([f(x) for x in xs])\n",
    "  plt.xlabel('iteration')\n",
    "  plt.ylabel('f(x)')\n",
    "  plt.title('Progress of gradient descent')\n",
    "  plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1588,
     "status": "ok",
     "timestamp": 1599418997321,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "1hCdjCNsctK3",
    "outputId": "869661f8-17d7-42f8-cbea-fc2bedcfb575"
   },
   "outputs": [],
   "source": [
    "# plot progress of gradient descent for function f\n",
    "x, xs = grad_descent(f, 2)\n",
    "plot_descent(f, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bpU22BtHFMa"
   },
   "source": [
    "## Binary Logistic regression\n",
    "\n",
    "The key idea for training is that we want to use gradient descent to find the model parameters that minimize the loss function.  For binary classification with logistic regression, the loss function is \"log loss\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart disease data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/grbruns/cst383/master/heart.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['output'] -= 1         # 0 = ok, 1 = heart disease\n",
    "\n",
    "predictors = ['age', 'restbp', 'maxhr']\n",
    "target = 'output'\n",
    "X = df[predictors].values\n",
    "y = df[target].values\n",
    "\n",
    "# scale the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# augment the data\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]        # add new first column of 1's\n",
    "\n",
    "print(X1.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1599418997759,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "5jgNm8YEGO3-",
    "outputId": "4120880b-14b1-41f9-8f41-9e55743d735b"
   },
   "outputs": [],
   "source": [
    "# plot output by rest BP\n",
    "plt.scatter(X[:,1], y, );\n",
    "plt.title('training data set')\n",
    "plt.xlabel('resting blood pressure (scaled)')\n",
    "plt.ylabel('output');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_ZRCn_NEySC"
   },
   "source": [
    "### Log loss\n",
    "\n",
    "Note that to compute the loss we need both the model parameters and the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement sigmoid yourself, not with a library function\n",
    "def sigmoid(x):\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sigmoid()\n",
    "\n",
    "print(sigmoid(-1.0))\n",
    "print(sigmoid(0))\n",
    "print(sigmoid(0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement log loss yourself, not with a library function\n",
    "def log_loss(b, X1, y):\n",
    "    # YOUR CODE HERE\n",
    "    # hints: \n",
    "    # - first make predictions, then compute loss\n",
    "    # - be sure to end with a return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2004,
     "status": "ok",
     "timestamp": 1599418997761,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "OJQ4FzAHKJ6_",
    "outputId": "7da64797-e43c-470c-e5eb-f1edf80044ef"
   },
   "outputs": [],
   "source": [
    "# test log_loss() by computing the loss for various\n",
    "# values of linear parameters b\n",
    "m = 4\n",
    "bs = np.array([[-0.2, 0.1, 0.3, -1.0],\n",
    "              [0.8, 0.5, 0.6, 0.9],\n",
    "              [0.05, 0.08, 0.02, 0.8]])\n",
    "for i in range(bs.shape[0]):\n",
    "    b = bs[i]\n",
    "    print('b: {}, log_loss(b): {:0.3f}'.format(b, log_loss(b, X1, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put together our gradient descent function and log_loss function to perform logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1599418997762,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "69son0weHcjt",
    "outputId": "52798d4c-8c30-4862-f49c-718a16665fb3"
   },
   "outputs": [],
   "source": [
    "# hints:\n",
    "# for gradient descent, the loss function will be log_loss() with the \n",
    "# training data X1, y \"hardwired\" in\n",
    "\n",
    "loss = lambda b: # YOUR CODE HERE\n",
    "b_estimated, history = grad_descent(# YOUR CODE HERE)\n",
    "print(b_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9s_kno5mjzjo"
   },
   "source": [
    "Compute training loss and training accuracy.  Use a threshold of 0.5 when computing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2362,
     "status": "ok",
     "timestamp": 1599418998144,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "EabHFq8Djxvf",
    "outputId": "c5fbc5ec-5bfb-4a2c-b488-aeda6af2d1ca"
   },
   "outputs": [],
   "source": [
    "pred = sigmoid(X1.dot(b_estimated))    \n",
    "training_loss = loss(b_estimated)\n",
    "training_acc = (y == (pred > 0.5)).mean()\n",
    "\n",
    "print('training loss: {:0.3f}, training accuracy: {:0.3f}'.format(training_loss, training_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows how log loss decreases during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1989,
     "status": "ok",
     "timestamp": 1599418997762,
     "user": {
      "displayName": "Glenn Bruns",
      "photoUrl": "",
      "userId": "02927676052174595786"
     },
     "user_tz": 420
    },
    "id": "DGMwUljzNHWY",
    "outputId": "0a2f5002-5ef1-42b7-8fa4-6b5b43d507ea"
   },
   "outputs": [],
   "source": [
    "plot_descent(loss, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to the result from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('Model coefficients:')\n",
    "print(clf.intercept_)\n",
    "print(clf.coef_)\n",
    "\n",
    "print('\\nScikit-Learn training accuracy: {:0.3f}'.format(clf.score(X,y)))     # training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a multi-class classification problem, then we need to use a separate linear model for each output class.  If we have 3 output class, for example, from each input we will get a vector of three output values.  In multi-class logistic regression, we transform this vector to a vector of probabilities using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/grbruns/cst383/master/iris.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[:4]\n",
    "X = df[predictors].values\n",
    "y_raw = df['species'].values\n",
    "\n",
    "# one-hot encoding of the target\n",
    "df = pd.get_dummies(df)\n",
    "target = df.columns[4:]\n",
    "y = df[target].values\n",
    "\n",
    "# scale the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# augment the data\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]        # add new first column of 1's\n",
    "\n",
    "print(X1.shape)\n",
    "print(y.shape)\n",
    "print(y_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implement softmax yourself\n",
    "def softmax(x):\n",
    "    # YOUR CODE HERE\n",
    "    # don't forget the return statement at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax(np.array([1.7, 2.4, 0.5])))\n",
    "print(softmax(np.array([-0.2, 1.2, -0.1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a separate linear model for each output class.  If we have k output classes, and n predictors, then we need n+1 parameters for each class.  Instead of keeping the parameters for each class separate, we can put them in a single matrix B.  Each row of B will contain the parameters for one class.  In other words, B will have k rows and n+1 columns.\n",
    "\n",
    "Think about the matrix multiplication that is needed here.  Previously we got a single output value for each training example by computing X1.dot(b).  How shall we combine X1 and B to get an array of outputs, one for each class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using a random set of parameters of the linear model.  Each row in array pred is a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.random.rand(X1.shape[1], y.shape[1])\n",
    "pred = X1.dot(B)\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply softmax to each row, so that the values in each row sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = np.apply_along_axis(softmax, 1, pred)\n",
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of log loss to more than 2 classes is cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, y):\n",
    "    \"\"\" cross-entropy of arrays pred and y, where y is one-hot encoded \"\"\"\n",
    "    return # YOUR CODE HERE\n",
    "\n",
    "def cross_entropy_loss(b, X1, y):\n",
    "    B = b.reshape((X1.shape[1], y.shape[1]))\n",
    "    # YOUR CODE HERE\n",
    "    # hints: \n",
    "    # - note that the parameters are passed in as a 1D array, but\n",
    "    #   then reshaped\n",
    "    # - as with log loss, first compute the predictions\n",
    "    #   (see code above that shows how to make predictions)\n",
    "    # - don't forget the return statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cross_entropy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_entropy(np.array([0.98, 0.01, 0.01]), np.array([1, 0, 0])))\n",
    "print(cross_entropy(np.array([0.90, 0.05, 0.05]), np.array([1, 0, 0])))\n",
    "print(cross_entropy(np.array([0.40, 0.20, 0.20]), np.array([1, 0, 0])))\n",
    "print(cross_entropy(np.array([0.40, 0.20, 0.20]), np.array([0, 1, 0])))\n",
    "\n",
    "# class examples\n",
    "print(cross_entropy(np.array([0.3, 0.61, 0.09]), np.array([0, 1, 0])))\n",
    "print(cross_entropy(np.array([0.3, 0.61, 0.09]), np.array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking an example from lecture\n",
    "print(cross_entropy(np.array([[0.1, 0.5, 0.4],\n",
    "                              [0.1, 0.1, 0.8],\n",
    "                              [0.2, 0.6, 0.2]]),\n",
    "                    np.array([[1, 0, 0],\n",
    "                              [0, 0, 1],\n",
    "                              [0, 1, 0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cross_entropy_loss()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of b equals the number of columns in X1\n",
    "# times the number of target classes\n",
    "n = X1.shape[1] * y.shape[1]\n",
    "\n",
    "b = np.linspace(0, 1, n)\n",
    "print(cross_entropy_loss(b, X1, y))\n",
    "\n",
    "b = np.linspace(-2, 2, n)\n",
    "print(cross_entropy_loss(b, X1, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with cross-entropy.\n",
    "\n",
    "Using gradient descent, softmax, and cross entropy loss we can perform multi-class logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss = # YOUR CODE HERE\n",
    "num_parameters = # YOUR CODE HERE\n",
    "b_estimated, history = grad_descent(loss, num_parameters, alpha=0.5, n_iterations=400, threshold=0.00001)\n",
    "print(b_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_entropy_loss(b_estimated, X1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descent(loss, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute training set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = b_estimated.reshape((X1.shape[1], y.shape[1]))\n",
    "pred = np.apply_along_axis(np.argmax, 1, X1.dot(B1))  \n",
    "y1d  = np.apply_along_axis(np.argmax, 1, y)\n",
    "\n",
    "# training accuracy\n",
    "print('gradient descent training accuracy: {:0.3f}'.format((pred == y1d).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results to Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y_raw)\n",
    "\n",
    "print('Scikit-Learn training accuracy: {:0.3f}'.format(clf.score(X,y_raw)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "grad_desc_1d-master.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
