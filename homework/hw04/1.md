# Reading: Gradient Descent Basics

## Instructions

Read chapter 4 of the Géron book up to, but not including, the section on Stochastic Gradient Descent.
Answer the following questions by editting [grad-desc.txt](grad-desc.txt) (and leaving the name the same).
Some of these questions are based on material from lectures, just just on Géron's book.

## Questions

### Q1
(a/b)
What would be a situation in which you'd think of using gradient descent with linear regression (as opposed to using the normal equation)?
a.  when you have a large number of training examples
b.  when you have a large number of predictor variables

### Q2 
(a/b/c)
How much of the training data is used in a single step of batch gradient descent?
a.  all of it
b.  it depends on the learning rate
c.  it depends on the number of iterations

### Q3
(True/False)
With linear regression, the Normal Equation is guaranteed to give the parameter values that minimize MSE.


### Q4
(True/False)
The purpose of gradient descent is to find the maximum (or minimum) value of a function.

### Q5
(Yes/No)
Will changing the starting point of gradient descent possibly impact the result it returns?

### Q6
(True/False)
With linear regression, there is a danger of gradient descent reaching a point that gives a local minima, not the global minimum.

### Q7
(True/False)
Linear regression and logistic regression use the same loss function.
