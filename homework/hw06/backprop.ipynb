{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e5d2a8",
   "metadata": {
    "id": "33e5d2a8"
   },
   "source": [
    "# Backprop for linear regression and beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b59e16",
   "metadata": {
    "id": "92b59e16"
   },
   "source": [
    "In this assignment you'll write backprop code for a few simple neural nets. The first one implements linear regression, but we also work with neural nets having a hidden layer of neurons.\n",
    "\n",
    "You'll also tweak the learning rate and number of training steps to try to get your nets to perform well.\n",
    "\n",
    "The understanding you get of the how neural nets work will help a lot in the coming weeks.\n",
    "\n",
    "vf22.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5b83d",
   "metadata": {
    "id": "61b5b83d"
   },
   "source": [
    "#### Instructions:\n",
    "\n",
    "- This homework is to be done completely on your own.  Don't refer to the web for solutions and don't discuss the homework with others.\n",
    "- Start by reading the notebook.\n",
    "- There are 7 problems.  For each problem, add code to the indicated notebook cell.\n",
    "- Do not modify any other cells, and do not add imports.\n",
    "- It is not enough to duplicate my output -- your code must solve the problem as stated.\n",
    "- Be sure to \"restart and run all\" before submitting your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd422c0f",
   "metadata": {
    "id": "dd422c0f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a802b7",
   "metadata": {
    "id": "d6a802b7"
   },
   "outputs": [],
   "source": [
    "def plot_loss(zs, ymax=10000):\n",
    "    \"\"\" Plot loss values over time.  zs is a 1D array or list of loss values. \"\"\"\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    n = zs.size\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(zs)\n",
    "    plt.title('loss over time')\n",
    "    plt.ylim(0, ymax)\n",
    "    plt.xlabel('training step')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01e0e8",
   "metadata": {
    "id": "7a01e0e8"
   },
   "source": [
    "Feel free to use the functions below in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad47f9",
   "metadata": {
    "id": "9dad47f9"
   },
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "    return 0.5 * (y - y_pred)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def neuron(w, x):\n",
    "    \"\"\" Summation followed by a sigmoid activation function.\n",
    "        w is a 1D NumPy array\n",
    "        x is a 1D or 2D NumPy array\n",
    "    \"\"\"\n",
    "    return sigmoid(x.dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ba2ca",
   "metadata": {
    "id": "f77ba2ca"
   },
   "source": [
    "#### Heart disease data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d99eb",
   "metadata": {
    "id": "354d99eb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/grbruns/cst383/master/heart.csv')\n",
    "df['output'] = df['output'] - 1\n",
    "\n",
    "predictors = ['restbp', 'age']\n",
    "target = 'maxhr'\n",
    "\n",
    "X = df[predictors].values\n",
    "y = df[target].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7d09b",
   "metadata": {
    "id": "33f7d09b"
   },
   "source": [
    "## Linear regression, 2 inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7318e3",
   "metadata": {
    "id": "8b7318e3"
   },
   "source": [
    "The figure below shows linear regression written as a tiny neural net.  There are two inputs, x1 and x2, along with a bias input.  The true target value is y, and the predicted target value is v.  The value of the loss function is z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6031f",
   "metadata": {
    "id": "66a6031f"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/grbruns/cst495/master/lin-reg-net.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e632ad",
   "metadata": {
    "id": "e7e632ad"
   },
   "source": [
    "Writing the network as equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efb715",
   "metadata": {
    "id": "e6efb715"
   },
   "outputs": [],
   "source": [
    "# v = b0 + b1*x1 + b2*x2  (y predicted)\n",
    "# z = mse(v, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93714071",
   "metadata": {
    "id": "93714071"
   },
   "source": [
    "### Problem 1\n",
    "\n",
    "This problem is like a lab problem we worked on in class.\n",
    "\n",
    "Add your code at the marked locations.  You can use whatever variable names you like, but you will need to use b0, b1, b2 for the weights.  Use no loops or dictionaries.  Do not remove my comments.\n",
    "\n",
    "I recommend using the names in the diagram above.  For the partial derivatives, I used names like dv_b0 for the partial derivative of v with respect to b0 and dz_b2 for the partial derivatives of z with respect to b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59663d",
   "metadata": {
    "id": "4e59663d"
   },
   "outputs": [],
   "source": [
    "def train(X, y, alpha, num_iterations):\n",
    "\n",
    "    m, n = X.shape\n",
    "\n",
    "    # initialize parameters\n",
    "    b0, b1, b2 = np.random.rand(3) - 0.5\n",
    "\n",
    "    z_history = []\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # stochastic gradient descent; get a random training example\n",
    "        i = np.random.choice(m)\n",
    "        x1 = X[i,0]\n",
    "        x2 = X[i,1]\n",
    "\n",
    "        # forward prop (be sure to include variable z)\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # backprop\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # update parametersk\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        z_history.append(z)\n",
    "\n",
    "    return np.array([b0,b1,b2]), z_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44f3b0",
   "metadata": {
    "id": "9f44f3b0"
   },
   "source": [
    "### Problem 2\n",
    "\n",
    "Set the values of alpha (learning rate) and num_iterations below such that you get a good result when training.  You can see what a good result is by comparing to LinearRegression below.  \n",
    "\n",
    "Because stochastic gradient descent is being used, there is a lot of noise, and the result you get will depend on the run.  Don't waste time \"chasing the noise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd98070",
   "metadata": {
    "id": "1bd98070"
   },
   "outputs": [],
   "source": [
    "alpha = None            # replace None with your value\n",
    "num_iterations = None   # replace None with your value\n",
    "b, z_history = train(X, y, alpha, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478051d6",
   "metadata": {
    "id": "478051d6"
   },
   "source": [
    "Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d04a4",
   "metadata": {
    "id": "450d04a4"
   },
   "outputs": [],
   "source": [
    "print(f'training result: {np.round(b, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1951820",
   "metadata": {
    "id": "b1951820"
   },
   "source": [
    "#### Plot loss over time\n",
    "\n",
    "Looking at the change in loss over time is helpful when setting up learning rate and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dea45c",
   "metadata": {
    "id": "49dea45c"
   },
   "outputs": [],
   "source": [
    "plot_loss(z_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa5a24",
   "metadata": {
    "id": "38fa5a24"
   },
   "source": [
    "Compute the training MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3adc65",
   "metadata": {
    "id": "5d3adc65",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = b[0] + b[1]*X[:,0] + b[2]*X[:,1]\n",
    "net_mse = ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Training MSE for neural net: {net_mse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4927a25",
   "metadata": {
    "id": "a4927a25"
   },
   "source": [
    "Compare to the training MSE obtained with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01997ba6",
   "metadata": {
    "id": "01997ba6"
   },
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X, y)\n",
    "y_pred = regr.predict(X)\n",
    "\n",
    "print(f'Training MSE for linear regression: {((y - y_pred)**2).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85405d0f",
   "metadata": {
    "id": "85405d0f"
   },
   "source": [
    "Coefficients from linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989689d6",
   "metadata": {
    "id": "989689d6"
   },
   "outputs": [],
   "source": [
    "print(f'coefficients: {np.array([ regr.intercept_ ] + list(regr.coef_)).round(3) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88374a7f",
   "metadata": {
    "id": "88374a7f"
   },
   "source": [
    "## Linear regression, any number of inputs\n",
    "\n",
    "It's pretty easy to tweak your result for problem 1 so that it will work with any number of inputs.\n",
    "\n",
    "Add your code in the cell below at the marked locations.  Use no loops or dictionaries.\n",
    "\n",
    "Note that the number of inputs can be determined from input 2D array X.\n",
    "\n",
    "Make sure you code can work with 3 or more inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a2aaf",
   "metadata": {
    "id": "ae3a2aaf"
   },
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b649eb",
   "metadata": {
    "id": "94b649eb"
   },
   "outputs": [],
   "source": [
    "def train2(X, y, alpha, num_iterations):\n",
    "\n",
    "    # augment X\n",
    "    m,n = X.shape\n",
    "    Xa = np.c_[np.ones(m), X]\n",
    "\n",
    "    # initialize parameters\n",
    "    b = np.random.rand(n+1) - 0.5\n",
    "\n",
    "    z_history = []\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # stochastic gradient descent; get a random training example\n",
    "        i = np.random.choice(m)\n",
    "        x = Xa[i]\n",
    "\n",
    "        # forward prop\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # backprop\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # update parameters\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        z_history.append(z)\n",
    "\n",
    "    return b, z_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a93a86",
   "metadata": {
    "id": "94a93a86"
   },
   "source": [
    "### Problem 4\n",
    "\n",
    "Set the values of alpha (learning rate) and num_iterations below to get a good result when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2e206",
   "metadata": {
    "id": "efe2e206"
   },
   "outputs": [],
   "source": [
    "alpha = None              # replace None with your value\n",
    "num_iterations = None     # replace None with your value\n",
    "b, z_history = train2(X, y, alpha, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703dd8f9",
   "metadata": {
    "id": "703dd8f9"
   },
   "outputs": [],
   "source": [
    "print(f'training result: {np.round(b, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdeca89",
   "metadata": {
    "id": "ecdeca89"
   },
   "outputs": [],
   "source": [
    "plot_loss(z_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11621b",
   "metadata": {
    "id": "4f11621b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = b[0] + b[1]*X[:,0] + b[2]*X[:,1]\n",
    "net_mse = ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Training MSE for neural net: {net_mse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c564013",
   "metadata": {
    "id": "5c564013"
   },
   "source": [
    "## Linear regression plus a hidden layer of 2 neurons, each with sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a382445",
   "metadata": {
    "id": "3a382445"
   },
   "source": [
    "The figure below is a tiny neural net for regression.  It has a single hidden layer with two neurons.  The neurons use a sigmoid activation function (which isn't a common choice for modern neural nets).\n",
    "\n",
    "In writing backprop code it's important to have a clear scheme for naming the weights and partial derivatives in the network.\n",
    "\n",
    "The figure shows the names I used.  Array x is used for the inputs to neuron 1 (the top neuron), and neuron 2.  Note that x[0], the first input to neurons 1 and 2, is a bias input -- it always has value 1.\n",
    "\n",
    "Similarly, xs is the array of inputs to the summation node.\n",
    "\n",
    "For the partial derivatives, I use px1[0] for the partial derivative of xs[1] with respect to z.  In other words, I just use a p in front of the input name.  We can associate the names of the partial derivatives with the names of the inputs because every partial derivative is with respect to the loss.\n",
    "\n",
    "Your code should handle the case where there are any number of inputs to the network, not just 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657736ea",
   "metadata": {
    "id": "657736ea"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/grbruns/cst495/master/tiny-net.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b2d4b",
   "metadata": {
    "id": "390b2d4b"
   },
   "outputs": [],
   "source": [
    "# Summary of variable names\n",
    "#\n",
    "# variables associated with neuron 1:\n",
    "# inputs for neuron 1:             x1[0],  x1[1],  x1[2]\n",
    "# weights for neuron 1:            w1[0],  w1[1],  w1[2]\n",
    "# partials for neuron 1 inputs:   px1[0], px1[1], px1[2]\n",
    "# partials for neuron 2 weights:  pw1[0], pw1[1], pw1[2]\n",
    "#\n",
    "# variables associated with the summation node:\n",
    "# inputs:  xs[0], xs[1], xs[2]\n",
    "# weights: ws[0], ws[1], ws[2]\n",
    "# partials for the inputs:  pxs[0], pxs[1], pxs[2]\n",
    "# partials for the weights: pws[0], pws[1], pws[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d42f9",
   "metadata": {
    "id": "f67d42f9"
   },
   "source": [
    "### Problem 5\n",
    "\n",
    "Add your code in the cell below at the marked locations.\n",
    "\n",
    "What makes backprop different here is that both the summation node and the neurons need to have their weights updated.  You should update the weights of the summation node right after you compute the partial derivatives for that node.\n",
    "\n",
    "Use no loops or dictionaries in the code you add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901469d",
   "metadata": {
    "id": "d901469d"
   },
   "outputs": [],
   "source": [
    "def train3(X, y, alpha, num_iterations):\n",
    "\n",
    "    num_neurons = 2\n",
    "    m,n = X.shape\n",
    "    # augmented version of X\n",
    "    Xa = np.c_[np.ones(m), X]\n",
    "\n",
    "    # initialize parameters of the neural net\n",
    "    w1 = np.random.rand(n+1) - 0.5\n",
    "    w2 = np.random.rand(n+1) - 0.5\n",
    "    ws = np.random.rand(num_neurons + 1) - 0.5\n",
    "\n",
    "    z_history = []\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # stochastic gradient descent; get a random training example\n",
    "        i = np.random.choice(m)\n",
    "        x = Xa[i]\n",
    "\n",
    "        #\n",
    "        # forward prop\n",
    "        #\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # be sure to set loss variable z\n",
    "\n",
    "        #\n",
    "        # backprop\n",
    "        #\n",
    "\n",
    "        # MSE node\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # summation node: compute partials and update ws\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # node 1: compute partials and update w1\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # node 2: compute partials and update w2\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        z_history.append(z)\n",
    "\n",
    "    return w1, w2, ws, z_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b48d6",
   "metadata": {
    "id": "5f6b48d6"
   },
   "source": [
    "### Problem 6\n",
    "\n",
    "Making predications is a little more complicated now that multiple nodes have weights.  Therefore, we will create a prediction function that will take all the network weights, plus input array X, and produce predictions.\n",
    "\n",
    "Add your code in the cell below at the marked locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07f6a5",
   "metadata": {
    "id": "db07f6a5"
   },
   "outputs": [],
   "source": [
    "def predict3(X, w1, w2, ws):\n",
    "    \"\"\" Compute predictions for the network with weight arrays w1, w2, and ws. \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # It is fine to copy/paste/adapt code from the previous problem.\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb70302",
   "metadata": {
    "id": "8eb70302"
   },
   "source": [
    "### Problem 7\n",
    "\n",
    "Set the values of alpha (learning rate) and num_iterations below such that you get a good result when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f5e03",
   "metadata": {
    "id": "a24f5e03"
   },
   "outputs": [],
   "source": [
    "alpha = None              # replace None with your value\n",
    "num_iterations = None     # replace None with your value\n",
    "w1, w2, ws, z_history = train3(X, y, alpha, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac337d8",
   "metadata": {
    "id": "3ac337d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss(z_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876fcf2",
   "metadata": {
    "id": "d876fcf2"
   },
   "source": [
    "#### Compute MSE on the training set\n",
    "\n",
    "Don't use the mse() function here, as it only works on pair of vectors, and uses the 0.5 factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dcfd6",
   "metadata": {
    "id": "795dcfd6"
   },
   "outputs": [],
   "source": [
    "y_pred = predict3(X, w1, w2, ws)\n",
    "print(f'Training MSE with neural net: {((y - y_pred)**2).mean():.3f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
