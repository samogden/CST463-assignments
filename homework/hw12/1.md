# Reading: Recurrent Neural networks
Homework policy reminder.  Do not use Google, ChatGPT, or people to get answers to questions.

## Instructions. 

Read Chapter 10 of our Chollet text.  Remember that I don't expect you to be able to answer questions about the details of LSTM layers.  You may need to also refer to lecture slides and the Keras documentation.  Optionally, read Chapter 15 of our GÃ©ron text, but skip the section on "Forecasting a time series".  You may also like to refer to Chapter 19 of Glassner's Deep Learning text.

## Questions 
Answer the following questions by editing file [rnn.txt](rnn.txt)

### Q1
Suppose we have a simple RNN layer with only one neuron, and that its input shape is (10, 1).  What is the shape of the output of the layer?  Assume that it only outputs its final state.  (Saying "a single neuron" means that units=1 in the SimpleRNN layer of Keras.)  

### Q2
Suppose we have a simple RNN layer with 10 neurons, and that its input shape is (10, 8).  How many parameters in the RNN layer?  Include bias weights in this and later questions.

### Q3
(Y/N)  Does the number of parameters in a simple RNN layer depend on the number of time steps in the input to the layer?

### Q4
(Y/N)  Does the number of parameters in a simple RNN layer depend on the number of channels in the input to the layer?

### Q5
(Y/N)  If you replace a simple RNN layer in a Keras model with a GRU layer, does the number of model parameters change?

### Q6
I have a simple RNN with a single neuron.  Its input weight is 0.5, the feedback (or "state") weight is 0.1, and the bias weight is 0.2.  It takes as input a sequence of length three, with a single input channel.  If the input sequence is `[1, 2, 0]`, what is the output of the RNN?  Your answer will be a single number.  Use a tanh activation.

### Q7
(Y/N)  Do the ReLU and tanh activation functions have the same minimum output value?
