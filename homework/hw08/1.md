# Reading: Training neural nets

## Instructions. 

Read the first paragraphs of Chapter 11 of our Géron text, and also these sections of the chapter: 
- The Vanishing/Exploding Gradients Problem
- Faster Optimizers.  

Answer the following questions by editing file [training.txt](training.txt).
Some of these questions are based on material from lectures, not Géron's book.

## Questions

### Q1

(a/b/c) What is the point of adjusting the learning rate?  Select all that apply.

a. improving predictive performance (for example, improved validation accuracy)

b. controlling overfitting

c. reducing training time

### Q2
In the Géron text, a function named exponential_decay_fn is defined.  
The function is used this way in the book:

```python
lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])
```

Look at the definition of `exponential_decay_fn` in the text.  
Using that definition and the code shown above, what will be the learning rate on epoch 15?

### Q3
What is the default learning rate for the SGD optimizer in Keras?

### Q4
(a/b/c) When is batch normalization applied?

a.	during training

b.	during testing

c.	during both training and testing


### Q5
(a/b/c/d) What relationship between time per training step and number of training steps do we expect with batch normalization?

a.	time per training step is increased, but total number of training steps decreases

b.	time per training step is reduced, but total number of training steps increases

c.	both time per training step and number of training steps decrease

d.	both time per training step and number of training steps increase


### Q6
(a/b) If you set your optimizer learning rate, you should expect:

a.	to use a lower learning rate if you use batch normalization

b.	to use a higher learning rate if you use batch normalization
