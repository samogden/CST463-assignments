# Finding derivatives analytically




<style type="text/css">
    ol { list-style-type: upper-alpha; }
</style>




## Purpose: 

We are learning how to do machine learning as optimization.  This will allow us to implement lots of machine learning algorithms by hand, including backprop in neural nets.  By implementing backprop ourselves we will get a solid understanding of how neural nets work.

## Instructions:  

Answer the problems below by editing the file derivs.txt with a text editor.  Be sure that you save your edited file as a .txt file -- not a pdf file, Word file, etc.

## Background:

Here are the rules we covered for differentiation of single-variable functions:

| Name               | Function               | Derivative                       |
|--------------------|------------------------|----------------------------------|
| constant           | $`f(x) = c`$           | $`f'(x) = 0`$                    |
| constant multiple  | $`g(x) = cf(x)`$       | $`g'(x) = cf'(x)`$               |
| power rule         | $`f(x) = x^{n}`$       | $`f(x) = nx^{n-1}`$              |
| sum and difference | $`h(x) = f(x) + g(x)`$ | $`h'(x) = f'(x) + g'(x)`$        |
| product rule       | $`h(x) = f(x)g(x)`$    | $`h(x) = f'(x)g(x) + f(x)g'(x)`$ |
| chain rule         | $`h(x) = f(g(x))`$     | $`h(x) = f'(g(x))g'(x)`$         |

## Questions

### Q1

(a/b/c/d)
$`f(x) = 2(x + 3)`$.
What is $`f'(x)`$?

1. $`2x`$
2. $2$
3. $`x + 3`$
4. $`1 + 3`$

### Q2

(a/b/c/d)
$`f(x) = -x + 3x^2`$

1. $`1 - 6x^2`$
2. $`6x`$
3. $`-1 + 6x`$
4. $`5x`$




